import pandas as pd
import re
import random
from collections import defaultdict
import math
import pickle

# ============================
# STEP 1: Load Data
# ============================
df = pd.read_csv('yelp_reviews.csv')

# Rename or use correct columns
# Assuming dataset has columns: 'Rating' and 'ReviewText'
# Create a binary target column: Liked = 1 (positive) or 0 (negative)
def map_rating(r):
    if r >= 4:
        return 1
    elif r <= 2:
        return 0
    else:
        return None  # Ignore neutral reviews

df['Liked'] = df['Rating'].apply(map_rating)
df = df.dropna(subset=['Liked'])  # remove neutral reviews

# ============================
# STEP 2: Clean the text
# ============================
def clean_text(text):
    # Convert to lowercase and keep only alphabets
    text = re.sub(r'[^a-zA-Z]', ' ', str(text)).lower().split()
    return text

df['Cleaned'] = df['ReviewText'].apply(clean_text)

# ============================
# STEP 3: Split into train/test
# ============================
data = list(zip(df['Cleaned'], df['Liked'].astype(int)))
random.seed(42)
random.shuffle(data)

split_idx = int(0.8 * len(data))
train_data = data[:split_idx]
test_data = data[split_idx:]

# ============================
# STEP 4: Train Naive Bayes
# ============================
word_counts = {
    0: defaultdict(int),  # Negative
    1: defaultdict(int),  # Positive
}
class_counts = {0: 0, 1: 0}
total_words = {0: 0, 1: 0}

for words, label in train_data:
    class_counts[label] += 1
    for word in words:
        word_counts[label][word] += 1
        total_words[label] += 1

# Vocabulary
vocab = set(word for label in word_counts for word in word_counts[label])
vocab_size = len(vocab)

# ============================
# STEP 5: Prediction function
# ============================
def predict_naive_bayes(words):
    scores = {}
    total_docs = sum(class_counts.values())
    for label in [0, 1]:
        # Prior probability
        log_prob = math.log((class_counts[label] + 1) / (total_docs + 2))  # Laplace on prior
        for word in words:
            word_freq = word_counts[label][word] + 1  # Laplace smoothing
            word_prob = word_freq / (total_words[label] + vocab_size)
            log_prob += math.log(word_prob)
        scores[label] = log_prob
    return 1 if scores[1] > scores[0] else 0

# ============================
# STEP 6: Evaluate
# ============================
correct = 0
for words, true_label in test_data:
    pred = predict_naive_bayes(words)
    if pred == true_label:
        correct += 1

accuracy = correct / len(test_data)
print(f"✅ Accuracy: {accuracy:.2f}")

# ============================
# STEP 7: Save Model Parameters
# ============================
# bundle them into a tuple
model_params = (word_counts, class_counts, total_words, vocab_size)

# save to a file
with open('model_params.pkl', 'wb') as f:
    pickle.dump(model_params, f)

print("✅ Model parameters saved to model_params.pkl")
